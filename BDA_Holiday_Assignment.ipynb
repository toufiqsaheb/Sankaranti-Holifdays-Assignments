{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35598878",
   "metadata": {},
   "source": [
    "# BDA (MR22-1CS0234) Holiday Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e26d6",
   "metadata": {},
   "source": [
    "## Question 1: Handling Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a043973d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import Required Libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Create synthetic imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1], \n",
    "                           n_informative=3, n_redundant=1, flip_y=0, \n",
    "                           n_features=5, n_clusters_per_class=1, n_samples=1000, random_state=42)\n",
    "\n",
    "# Before balancing\n",
    "print(\"Original class distribution:\", pd.Series(y).value_counts())\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# After balancing\n",
    "print(\"Balanced class distribution:\", pd.Series(y_smote).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370da9d2",
   "metadata": {},
   "source": [
    "## Question 2: Optimal Clusters for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296b10c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import Libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create synthetic dataset for clustering\n",
    "X_blobs, _ = make_blobs(n_samples=500, n_features=2, centers=4, random_state=42)\n",
    "\n",
    "# Elbow Method\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_blobs)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bca7a",
   "metadata": {},
   "source": [
    "## Question 3: Dimensionality Reduction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36499a56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import Libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Apply PCA\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Scatter plot for PCA projection\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.title('PCA Projection')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32218a",
   "metadata": {},
   "source": [
    "## Question 4: Correlations in a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'A': np.random.rand(100),\n",
    "    'B': np.random.rand(100),\n",
    "    'C': np.random.rand(100),\n",
    "    'D': np.random.rand(100)\n",
    "})\n",
    "\n",
    "# Compute correlations\n",
    "correlation_matrix = data.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750d73b",
   "metadata": {},
   "source": [
    "## Question 5: Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create data with missing values\n",
    "data_with_nans = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4],\n",
    "    'B': [np.nan, 2, 3, 4],\n",
    "    'C': [1, np.nan, np.nan, 4]\n",
    "})\n",
    "\n",
    "print(\"Original Data with NaNs:\")\n",
    "print(data_with_nans)\n",
    "\n",
    "# Imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data_with_nans), columns=data_with_nans.columns)\n",
    "\n",
    "print(\"Data after Imputation:\")\n",
    "print(data_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a277f",
   "metadata": {},
   "source": [
    "## Question 6: Detect and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with duplicates\n",
    "data_with_duplicates = pd.DataFrame({\n",
    "    'A': [1, 2, 2, 4],\n",
    "    'B': [5, 6, 6, 8],\n",
    "    'C': [9, 10, 10, 12]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data_with_duplicates)\n",
    "\n",
    "# Remove duplicates\n",
    "data_no_duplicates = data_with_duplicates.drop_duplicates()\n",
    "print(\"Data after removing duplicates:\")\n",
    "print(data_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97972fd0",
   "metadata": {},
   "source": [
    "## Question 7: Random Forest Regression for Housing Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abb9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create synthetic housing dataset\n",
    "X_housing, y_housing = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb280",
   "metadata": {},
   "source": [
    "## Question 8: Histogram, Bar Chart, and Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "data_chart = pd.DataFrame({\n",
    "    'Category': ['A', 'B', 'C'],\n",
    "    'Values': [30, 50, 20]\n",
    "})\n",
    "\n",
    "# Histogram\n",
    "plt.hist(np.random.randn(1000), bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Bar Chart\n",
    "plt.bar(data_chart['Category'], data_chart['Values'], color='green')\n",
    "plt.title('Bar Chart')\n",
    "plt.show()\n",
    "\n",
    "# Pie Chart\n",
    "plt.pie(data_chart['Values'], labels=data_chart['Category'], autopct='%1.1f%%')\n",
    "plt.title('Pie Chart')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d47bb",
   "metadata": {},
   "source": [
    "## Question 9: Linear and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2495102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation for Linear Regression\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "print(\"Linear Regression MSE:\", mean_squared_error(y_test, y_pred_linear))\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation for Logistic Regression\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8132df",
   "metadata": {},
   "source": [
    "## Question 10: Lag Features for Time-Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c77883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-series dataset\n",
    "\n",
    "time_series_data = pd.DataFrame({'Date': pd.date_range(start='1/1/2020', periods=10, freq='D'),\n",
    "                                 'Value': np.random.rand(10)})\n",
    "time_series_data['Lag_1'] = time_series_data['Value'].shift(1)\n",
    "time_series_data['Lag_2'] = time_series_data['Value'].shift(2)\n",
    "\n",
    "print(\"Time-Series Data with Lag Features:\")\n",
    "print(time_series_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
